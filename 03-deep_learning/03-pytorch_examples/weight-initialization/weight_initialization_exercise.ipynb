{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initialization\n",
    "In this lesson, you'll learn how to find good initial weights for a neural network. Weight initialization happens once, when a model is created and before it trains. Having good initial weights can place the neural network close to the optimal solution. This allows the neural network to come to the best solution quicker. \n",
    "\n",
    "<img src=\"notebook_ims/neuron_weights.png\" width=40%/>\n",
    "\n",
    "\n",
    "## Initial Weights and Observing Training Loss\n",
    "\n",
    "To see how different weights perform, we'll test on the same dataset and neural network. That way, we know that any changes in model behavior are due to the weights and not any changing data or model structure. \n",
    "> We'll instantiate at least two of the same models, with _different_ initial weights and see how the training loss decreases over time, such as in the example below. \n",
    "\n",
    "<img src=\"notebook_ims/loss_comparison_ex.png\" width=60%/>\n",
    "\n",
    "Sometimes the differences in training loss, over time, will be large and other times, certain weights offer only small improvements.\n",
    "\n",
    "### Dataset and Model\n",
    "\n",
    "We'll train an MLP to classify images from the [Fashion-MNIST database](https://github.com/zalandoresearch/fashion-mnist) to demonstrate the effect of different initial weights. As a reminder, the FashionMNIST dataset contains images of clothing types; `classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']`. The images are normalized so that their pixel values are in a range [0.0 - 1.0).  Run the cell below to download and load the dataset.\n",
    "\n",
    "---\n",
    "#### EXERCISE\n",
    "\n",
    "[Link to normalized distribution, exercise code](#normalex)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Load [Data](http://pytorch.org/docs/stable/torchvision/datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.FashionMNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.FashionMNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Some Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model Architecture\n",
    "\n",
    "We've defined the MLP that we'll use for classifying the dataset.\n",
    "\n",
    "### Neural Network\n",
    "<img style=\"float: left\" src=\"notebook_ims/neural_net.png\" width=50%/>\n",
    "\n",
    "\n",
    "* A 3 layer MLP with hidden dimensions of 256 and 128. \n",
    "\n",
    "* This MLP accepts a flattened image (784-value long vector) as input and produces 10 class scores as output.\n",
    "---\n",
    "We'll test the effect of different initial weights on this 3 layer neural network with ReLU activations and an Adam optimizer.  \n",
    "\n",
    "The lessons you learn apply to other neural networks, including different activations and optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initialize Weights\n",
    "Let's start looking at some initial weights.\n",
    "### All Zeros or Ones\n",
    "If you follow the principle of [Occam's razor](https://en.wikipedia.org/wiki/Occam's_razor), you might think setting all the weights to 0 or 1 would be the best solution.  This is not the case.\n",
    "\n",
    "With every weight the same, all the neurons at each layer are producing the same output.  This makes it hard to decide which weights to adjust.\n",
    "\n",
    "Let's compare the loss with all ones and all zero weights by defining two models with those constant weights.\n",
    "\n",
    "Below, we are using PyTorch's [nn.init](https://pytorch.org/docs/stable/nn.html#torch-nn-init) to initialize each Linear layer with a constant weight. The init library provides a number of weight initialization functions that give you the ability to initialize the weights of each layer according to layer type.\n",
    "\n",
    "In the case below, we look at every layer/module in our model. If it is a Linear layer (as all three layers are for this MLP), then we initialize those layer weights to be a `constant_weight` with bias=0 using the following code:\n",
    ">```\n",
    "if isinstance(m, nn.Linear):\n",
    "    nn.init.constant_(m.weight, constant_weight)\n",
    "    nn.init.constant_(m.bias, 0)\n",
    "```\n",
    "\n",
    "The `constant_weight` is a value that you can pass in when you instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_1=256, hidden_2=128, constant_weight=None):\n",
    "        super(Net, self).__init__()\n",
    "        # linear layer (784 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_1)\n",
    "        # linear layer (hidden_1 -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (hidden_2 -> 10)\n",
    "        self.fc3 = nn.Linear(hidden_2, 10)\n",
    "        # dropout layer (p=0.2)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # initialize the weights to a specified, constant value\n",
    "        if(constant_weight is not None):\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.constant_(m.weight, constant_weight)\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "            \n",
    "    def forward(self, x):\n",
    "        # flatten image input\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Behavior\n",
    "\n",
    "Below, we are using `helpers.compare_init_weights` to compare the training and validation loss for the two models we defined above, `model_0` and `model_1`.  This function takes in a list of models (each with different initial weights), the name of the plot to produce, and the training and validation dataset loaders. For each given model, it will plot the training loss for the first 100 batches and print out the validation accuracy after 2 training epochs. *Note: if you've used a small batch_size, you may want to increase the number of epochs here to better compare how models behave after seeing a few hundred images.* \n",
    "\n",
    "We plot the loss over the first 100 batches to better judge which model weights performed better at the start of training. **I recommend that you take a look at the code in `helpers.py` to look at the details behind how the models are trained, validated, and compared.**\n",
    "\n",
    "Run the cell below to see the difference between weights of all zeros against all ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize two NN's with 0 and 1 constant weights\n",
    "model_0 = Net(constant_weight=0)\n",
    "model_1 = Net(constant_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helpers\n",
    "\n",
    "# put them in list form to compare\n",
    "model_list = [(model_0, 'All Zeros'),\n",
    "              (model_1, 'All Ones')]\n",
    "\n",
    "\n",
    "# plot the loss over the first 100 batches\n",
    "helpers.compare_init_weights(model_list, \n",
    "                             'All Zeros vs All Ones', \n",
    "                             train_loader,\n",
    "                             valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the accuracy is close to guessing for both zeros and ones, around 10%.\n",
    "\n",
    "The neural network is having a hard time determining which weights need to be changed, since the neurons have the same output for each layer.  To avoid neurons with the same output, let's use unique weights.  We can also randomly select these weights to avoid being stuck in a local minimum for each run.\n",
    "\n",
    "A good solution for getting these random weights is to sample from a uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Uniform Distribution\n",
    "A [uniform distribution](https://en.wikipedia.org/wiki/Uniform_distribution) has the equal probability of picking any number from a set of numbers. We'll be picking from a continuous distribution, so the chance of picking the same number is low. We'll use NumPy's `np.random.uniform` function to pick random numbers from a uniform distribution.\n",
    "\n",
    ">#### [`np.random_uniform(low=0.0, high=1.0, size=None)`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.uniform.html)\n",
    ">Outputs random values from a uniform distribution.\n",
    "\n",
    ">The generated values follow a uniform distribution in the range [low, high). The lower bound minval is included in the range, while the upper bound maxval is excluded.\n",
    "\n",
    ">- **low:** The lower bound on the range of random values to generate. Defaults to 0.\n",
    "- **high:** The upper bound on the range of random values to generate. Defaults to 1.\n",
    "- **size:** An int or tuple of ints that specify the shape of the output array.\n",
    "\n",
    "We can visualize the uniform distribution by using a histogram. Let's map the values from `np.random_uniform(-3, 3, [1000])` to a histogram using the `helper.hist_dist` function. This will be `1000` random float values from `-3` to `3`, excluding the value `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helpers.hist_dist('Random Uniform (low=-3, high=3)', np.random.uniform(-3, 3, [1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram used 500 buckets for the 1000 values.  Since the chance for any single bucket is the same, there should be around 2 values for each bucket. That's exactly what we see with the histogram.  Some buckets have more and some have less, but they trend around 2.\n",
    "\n",
    "Now that you understand the uniform function, let's use PyTorch's `nn.init` to apply it to a model's initial weights.\n",
    "\n",
    "### Uniform Initialization, Baseline\n",
    "\n",
    "\n",
    "Let's see how well the neural network trains using a uniform weight initialization, where `low=0.0` and `high=1.0`. Below, I'll show you another way (besides in the Net class code) to initialize the weights of a network. To define weights outside of the model definition, you can:\n",
    ">1. Define a function that assigns weights by the type of network layer, *then* \n",
    "2. Apply those weights to an initialized model using `model.apply(fn)`, which applies a function to each model layer.\n",
    "\n",
    "This time, we'll use `weight.data.uniform_` to initialize the weights of our model, directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes in a module and applies the specified weight initialization\n",
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # apply a uniform distribution to the weights and a bias=0\n",
    "        m.weight.data.uniform_(0.0, 1.0)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new model with these weights\n",
    "model_uniform = Net()\n",
    "model_uniform.apply(weights_init_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate behavior \n",
    "helpers.compare_init_weights([(model_uniform, 'Uniform Weights')], \n",
    "                             'Uniform Baseline', \n",
    "                             train_loader,\n",
    "                             valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The loss graph is showing the neural network is learning, which it didn't with all zeros or all ones. We're headed in the right direction!\n",
    "\n",
    "## General rule for setting weights\n",
    "The general rule for setting the weights in a neural network is to set them to be close to zero without being too small. \n",
    ">Good practice is to start your weights in the range of $[-y, y]$ where $y=1/\\sqrt{n}$  \n",
    "($n$ is the number of inputs to a given neuron).\n",
    "\n",
    "Let's see if this holds true; let's create a baseline to compare with and center our uniform range over zero by shifting it over by 0.5.  This will give us the range [-0.5, 0.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes in a module and applies the specified weight initialization\n",
    "def weights_init_uniform_center(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # apply a centered, uniform distribution to the weights\n",
    "        m.weight.data.uniform_(-0.5, 0.5)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "# create a new model with these weights\n",
    "model_centered = Net()\n",
    "model_centered.apply(weights_init_uniform_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's create a distribution and model that uses the **general rule** for weight initialization; using the range $[-y, y]$, where $y=1/\\sqrt{n}$ .\n",
    "\n",
    "And finally, we'll compare the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# takes in a module and applies the specified weight initialization\n",
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "# create a new model with these weights\n",
    "model_rule = Net()\n",
    "model_rule.apply(weights_init_uniform_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare these two models\n",
    "model_list = [(model_centered, 'Centered Weights [-0.5, 0.5)'), \n",
    "              (model_rule, 'General Rule [-y, y)')]\n",
    "\n",
    "# evaluate behavior \n",
    "helpers.compare_init_weights(model_list, \n",
    "                             '[-0.5, 0.5) vs [-y, y)', \n",
    "                             train_loader,\n",
    "                             valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This behavior is really promising! Not only is the loss decreasing, but it seems to do so very quickly for our uniform weights that follow the general rule; after only two epochs we get a fairly high validation accuracy and this should give you some intuition for why starting out with the right initial weights can really help your training process!\n",
    "\n",
    "---\n",
    "\n",
    "Since the uniform distribution has the same chance to pick *any value* in a range, what if we used a distribution that had a higher chance of picking numbers closer to 0?  Let's look at the normal distribution.\n",
    "\n",
    "### Normal Distribution\n",
    "Unlike the uniform distribution, the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) has a higher likelihood of picking number close to it's mean. To visualize it, let's plot values from NumPy's `np.random.normal` function to a histogram.\n",
    "\n",
    ">[np.random.normal(loc=0.0, scale=1.0, size=None)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.normal.html)\n",
    "\n",
    ">Outputs random values from a normal distribution.\n",
    "\n",
    ">- **loc:** The mean of the normal distribution.\n",
    "- **scale:** The standard deviation of the normal distribution.\n",
    "- **shape:** The shape of the output array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helpers.hist_dist('Random Normal (mean=0.0, stddev=1.0)', np.random.normal(size=[1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the normal distribution against the previous, rule-based, uniform distribution.\n",
    "\n",
    "<a id='normalex'></a>\n",
    "#### TODO: Define a weight initialization function that gets weights from a normal distribution \n",
    "> The normal distribution should have a mean of 0 and a standard deviation of $y=1/\\sqrt{n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## complete this function\n",
    "def weights_init_normal(m):\n",
    "    '''Takes in a module and initializes all linear layers with weight\n",
    "       values taken from a normal distribution.'''\n",
    "    \n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model\n",
    "    # m.weight.data shoud be taken from a normal distribution\n",
    "    # m.bias.data should be 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## -- no need to change code below this line -- ##\n",
    "\n",
    "# create a new model with the rule-based, uniform weights\n",
    "model_uniform_rule = Net()\n",
    "model_uniform_rule.apply(weights_init_uniform_rule)\n",
    "\n",
    "# create a new model with the rule-based, NORMAL weights\n",
    "model_normal_rule = Net()\n",
    "model_normal_rule.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare the two models\n",
    "model_list = [(model_uniform_rule, 'Uniform Rule [-y, y)'), \n",
    "              (model_normal_rule, 'Normal Distribution')]\n",
    "\n",
    "# evaluate behavior \n",
    "helpers.compare_init_weights(model_list, \n",
    "                             'Uniform vs Normal', \n",
    "                             train_loader,\n",
    "                             valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal distribution gives us pretty similar behavior compared to the uniform distribution, in this case. This is likely because our network is so small; a larger neural network will pick more weight values from each of these distributions, magnifying the effect of both initialization styles. In general, a normal distribution will result in better performance for a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Automatic Initialization\n",
    "\n",
    "Let's quickly take a look at what happens *without any explicit weight initialization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Instantiate a model with _no_ explicit weight initialization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## evaluate the behavior using helpers.compare_init_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you complete this exercise, keep in mind these questions:\n",
    "* What initializaion strategy has the lowest training loss after two epochs? What about highest validation accuracy?\n",
    "* After testing all these initial weight options, which would you decide to use in a final classification model?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
