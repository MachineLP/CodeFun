有监督学习，带标注信息输入数据集，标注真实期望输出，训练推断模型，覆盖初始数据集，预测新样本输出。推断模型具体运算步骤代码设置。由给定问题解决模型确定。模型确定，运算固定。相关数据是模型参数，训练过程不断更新，模型学习，调整输出。

训练闭环一般结构：初始化模型参数(随机赋值或全0) -> 输入训练数据(样本及期望输出，随机打乱次序) ->训练数据执行推断模型(得到输出) -> 计算损失(输出与期望输出差距) ->调整模型参数(调整参数值，损失最小化，梯度下降算法)。依据学习速率、给定模型、输入数据，大量循环重复。

评估阶段，不同测试集(期望输出)模型推断，评估模型数据集损失，了解训练集外推广能力。原始数据集70%用于训练，30%用于评估。

初始化变量和模型参数，定义训练闭环运算。计算返回推断模型输出(数据X)，计算损失(训练数据X及期望输出Y)，读取或生成训练数据X及期望输出Y，训练或调整模型参数(计算总损失)，评估训练模型。会话对象启动数据流图，搭建流程。

训练模型，多个训练周期更新参数(变量)。tf.train.Saver类保存变量到二进制文件。周期性保存所有变量，创建检查点(checkpoint)文件，从最近检查点恢复训练。启动会话对象前实例化Saver对象，每完成1000次训练迭代或训练结束，调用tf.train.Saver.save方法，创建遵循命名模板my-model{step}检查点文件，保存每个变量当前值，默认只保留最近5个文件，更早的自动删除。tf. train.get_checkpoint_state方法验证有无检查点文件。tf. trainSaver.restore方法恢复变量值。检查是否有检查点文件存在，在开始训练闭环前恢复变量值，根据检查点文件名称恢复全局迭代次数。

线性回归，给定数据点集合训练集，找到最吻合线性函数。2D数据线性函数是一条直线。点代表训练数据，线代表模型推断结果。线性回归模型基本原理，Y为待预测值。X为一组独立预测变量样本。W为模型从训练数据学到的参数，每个变量权值。b也是模型从训练数据学到的参数，线性函数常量，模型偏置(bias)。

计算损失，总平方误差，模型对训练样本预测值与期望输出之差平方总和。预测输出向量与期望向量之间欧氏距离平方。2D数据集，总平方误差对应数据点垂直方向到所预测回归直线距离平方总和。L2范数或L2损失函数。采用平方避免计算平方根，节省计算量。

模型训练运算，梯度下降算法优化模型参数。损失函数值随训练步数增加逐渐减小。


    import tensorflow as tf
    import os
    #初始化变量和模型参数，定义训练闭环运算
    W = tf.Variable(tf.zeros([2, 1]), name="weights")#变量权值
    b = tf.Variable(0., name="bias")#线性函数常量，模型偏置
    def inference(X):#计算返回推断模型输出(数据X)
        print "function: inference"
        return tf.matmul(X, W) + b
    def loss(X, Y):#计算损失(训练数据X及期望输出Y)
        print "function: loss"
        Y_predicted = inference(X)
        return tf.reduce_sum(tf.squared_difference(Y, Y_predicted))
    def inputs():#读取或生成训练数据X及期望输出Y
        print "function: inputs"
        # Data from http://people.sc.fsu.edu/~jburkardt/datasets/regression/x09.txt
        weight_age = [[84, 46], [73, 20], [65, 52], [70, 30], [76, 57], [69, 25], [63, 28], [72, 36], [79, 57], [75, 44], [27, 24], [89, 31], [65, 52], [57, 23], [59, 60], [69, 48], [60, 34], [79, 51], [75, 50], [82, 34], [59, 46], [67, 23], [85, 37], [55, 40], [63, 30]]
        blood_fat_content = [354, 190, 405, 263, 451, 302, 288, 385, 402, 365, 209, 290, 346, 254, 395, 434, 220, 374, 308, 220, 311, 181, 274, 303, 244]
        return tf.to_float(weight_age), tf.to_float(blood_fat_content)
    def train(total_loss):#训练或调整模型参数(计算总损失)
        print "function: train"
        learning_rate = 0.0000001
        return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)
    def evaluate(sess, X, Y):#评估训练模型
        print "function: evaluate"
        print sess.run(inference([[80., 25.]]))# ~ 303
        print sess.run(inference([[65., 25.]]))# ~ 256
    saver = tf.train.Saver()#创建Saver对象
    #会话对象启动数据流图，搭建流程
    with tf.Session() as sess:
        print "Session: start"
        tf.global_variables_initializer().run()
        X, Y = inputs()
        total_loss = loss(X, Y)
        train_op = train(total_loss)
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(sess=sess, coord=coord)
        training_steps = 1000#实际训练迭代次数
        initial_step = 0
        checkpoin_dir = "./"
        ckpt = tf.train.get_checkpoint_state(os.path.dirname(checkpoin_dir))
        if ckpt and ckpt.model_checkpoint_path:
            print "checkpoint_path: " + ckpt.model_checkpoint_path
            saver.restore(sess, ckpt.model_checkpoint_path)#从检查点恢复模型参数
            initial_step = int(ckpt.model_checkpoint_path.rsplit('-', 1)[1])
        for step in range(initial_step, training_steps):#实际训练闭环
            sess.run([train_op])
        if step % 10 == 0:#查看训练过程损失递减
            print str(step)+ " loss: ", sess.run([total_loss])
            save_file = saver.save(sess, 'my-model', global_step=step)#创建遵循命名模板my-model-{step}检查点文件
            print str(step) + " save_file: ", save_file
        evaluate(sess, X, Y)#模型评估
        coord.request_stop()
        coord.join(threads)
        saver.save(sess, 'my-model', global_step=training_steps)
        print str(training_steps) + " final loss: ", sess.run([total_loss])
        sess.close()


参考资料：
《面向机器智能的TensorFlow实践》


