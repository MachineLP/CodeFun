自然语言处理 (NLP)问题都是序列化的。前馈神经网络，在单次前馈中对到来数据处理，假定所有输入独立，模式丢失。循环神经网络(recurrent neural network,RNN)对时间显式建模神经网络。RNN神经元可接收其他神经元加权输入。RNN神经元可与更高层建立连接，也可与更低层建立连接。隐含活性值在同一序列相邻输入间被记忆。2006年 LSTM。语音识别、语音合成、手写连体字识别、时间序列预测、图像标题生成、端到端机器翻译。

RNN由神经元和连接权值构成任意有向图。输入神经元(input neuron)拥有“到来”连接，活性值由输入数据设置。输出神经元(output neuron)是数据流图一组可读取预测结果神经元。所有其他神经元为隐含神经元(hidden neuron)。每个时间步，通过设置输入神经元为网络提供输入序列下一帧。隐含活性值作为下一个时间步附加输入。RNN当前隐含活性值为状态。序列最开始设置值0空状态。RNN状态依赖当前输入和上一状态。状态与序列所有前输入都间接相关，工作记忆(woring memory)。RNN权值矩阵定义执行程序，决定隐含活性值输入，不同活性值整合新活性值输出。sigmoid激活函数 RNN 2006年被证明图录完备(Turing-complete)。给定正确权值，RNN可完成任意计算程序相同计算。不存在找到完美权值方法，可用梯度下降法得到次好结果。

优化RNN，沿时间输展开，用优化前馈网络相同方式优化。复制序列神经元，连接在副本传递，移除循环边接而不改计算语义。相邻时间步权值相同强度。随时间反向传播(Back-Propagation Through Time,BPTT)，返回时间相关误差对权值(包括联结相邻副本权值)偏导。联结权值(tied weight)梯度相加。

循环神经网络常见映射：序列分类、序列生成、序列标注、序列翻译。序列标注(sequential labelling)，序列作为输入，训练网络为每帧数据产生正确输出，一个序列到另一个序列等长映射。序列分类(sequential classification)，每个序列输入对应一个类别标签，可仅选择上一帧输出训练RNN，更新权值时误差流经所有时间步收集集成有用信息。序列生成(sequential generation)，给定一个类别标签，输出反馈给网络作为下一步输入，生成序列。单个向量视为信息稠密表示。序列翻译(sequential translation)，域中序列编码，最后隐含活性值解码为另一个域中序列。输入输出概念层次有差异，两个不同RNN，第一个模型最后活性值初始化第二个模型。单个网络，序列后传入特殊符号输入，通知网络停止编码，开始解码。

带输出投影RNN网络结构，全连接隐含单元，映射输入输出。所有隐含单元都为输出，堆叠前馈层。隐含单元和输出单元不同激活函数。

TensorFlow支持RNN各种变体，tf.nn.rnn_cell。tensor flow.models.rnn中tf.nn.dynamic_rnn实现RNN动力学。接收循环网络定义，输入序列批数据。所有序列等长。返回保存每个时间步输出和隐含状态两个张量。从tensor flow.models.rnn导入rnn_cell和rnn。输入数据维数为batch_size*sequence_length*frame_size。不希望限制批次大小，第1维尺寸可以设None。rnn_cell.BasicRNNCell 创建基础RNN。rnn.dynamic_rnn 定义sequence_length步模拟RNN运算。定义RNN，沿时间轴展开，加载数据，选择TensorFlow优化器训练网络，tf.train.RMSPropOptimizer、tf.train.AdamOptimizer。

长时依赖性，网络记住含有许多后续不相关帧的序列第一帧。长序列，基础RNN展开网络深度非常大，层数非常多，每一层反向传播算法将来自网络上一层误差乘以局部偏导。如果大多数局部偏导远小于1,梯度每层变小，指数衰减，最终消失。如果很多偏导大于1,梯度值急剧增大。误差项包含相乘项权值矩阵转置。RNN相邻时间步联结一起，权值局部偏导都小于1或大于1,RNN每个权值都向相同方向缩放，梯度消失、爆炸问题突出。数值优化，浮点精度对梯度值产生影响。长短时记忆网络(long-short term memory,LSTM)RNN架构解决方案。

LSTM专门解决梯度消失、爆炸问题。学习长时依赖关系的RNN事实标准。将RNN普通神经元替换为内部拥有少量记忆LSTM单元(LSTM Cell)。联结一起，内部状态记忆时间步误差。LSTM内部状态有固定权值为1自连接，线性激活函数，局部偏导始终为1。反向传播，常量误差传输子(constant error carousel)在时间步携带误差不发生梯度消失或爆炸。内部状态随时间步传递误差，LSTM环绕门(surrounding gates)负责学习，非线性激活函数(sigmoid)。原始LSTM单元，一种门学习对到来活性值缩放，另一种门学习输出活性值缩放，学习包含或忽略新输入，学习给其他单元传递特征。单元输入送入不同权值门。可以把循环神经网络用为规模更大网络架构组成部分。
LSTMCell类可替换BasicRNNCell类，一个完整的LSTM层。输入非线性->输入门->状态->输出非线性->输出门。

LSTM流行变种，添加对内部循环连接比例缩放遗忘门(forget gate)，网络学会遗忘，内部循环连接局部偏导变成遗忘门活性值，可取非1值。当上下文重要，遗忘门保持关闭状态。输入非线性->输入门->状态->遗忘门->输出非线性->输出门。

添加窥视孔连接(peephole connection)，门能看到单元状态。当精确时间选择和间隔时有益。TensorFlow LSTM层传入use-peepholes=Trues标记激活窥视孔连接。

门限循环单元(Gated Recurrent Unit,GRU)，架构简单，更少计算量，没有输出门，输入和遗忘门整合单独更新门(update gate)。更新门决定内部状态与候选活性值融合比例。重置门(reset gate)和新输入确定部分隐含状态计算得到候选活性值。TensorFlow GRU层对应GRUCell类。只需要单元数目参数。输入->候选活性值->更新门->重置门->状态。

全连接隐含单元RNN，训练期间把不需要权值置0。最常见做法，两层或多层全连接RNN相互堆叠，信息只能在两层之间向上流动，多层RNN权值数目少，学习到更多抽象特征。


参考资料：
《面向机器智能的TensorFlow实践》


