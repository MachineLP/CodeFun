线性回归、对数几率回归模型，本质上是单个神经元。计算输入特征加权和。偏置视为每个样本输入特征为1权重，计算特征线性组合。激活(传递)函数 计算输出。线性回归，恒等式(值不变)。对数几率回归，sigmoid。输入->权重->求和->传递->输出。softmax分类含C个神经元，每个神经元对应一个输出类别。

XOR异或运算，无法通过线性模型解决。sigmoido类型神经元要求数据线性可分。2D数据存在直线，高维数据存在超平面，把不同类别样本分隔。

在神经网络输入和输出之间插入更多神经元，解决非线性可分问题。输入层->隐含层(hidden layer)->输出层->输出。隐含层使网络可以对输入数据提出更多问题。隐含层每个神经元对应一个问题，依据问题回答最终决定输出结果。隐含层在数据分布图允许神经网络绘制以一条以上分隔线。每条分隔线向输入数据划分提出问题，所有相等输出划分到单个区域。深度学习，添加更多隐含层，可采用不同类型连接，使用不同激活函数。

梯度下降法，找到函数极值点。学习，改进模型参数，大量训练，损失最小化。梯度下降法寻找损失函数极值点。梯度输出偏导数向量，每个分量对应函数对输入向量相应分量偏导。求偏导，当前变量外所有变量视为常数，用单变量求导法则。偏导数度量函数输出相对特定输入变量的变化率，当输入变量值变化，输出值的变化。损失函数输入变量指模型权值，不是实际数据集输入特征。相对于推断模型每个权值。
梯度输出向量表明每个位置损失函数增长最快方向，在函数每个位置向哪个方向移动函数值可增长。点表示权值当前值。梯度向右箭头表示为增加损失需向右移动，简头长度表示向右移动函数值增长量。反方向移动，损失函数值减少。直到梯度模为0,达到损失函数极小值点。
学习速率(learning rate)缩放梯度。梯度向量长度在损失函数单元中，缩放与权值相加。学习速率是超参数(hyperparameter)，模型手工可配置设置，需指定正确值。太小，需要多轮迭代。太大，超调(overshooting)，永远找不到极小值点。用tf.summary.scalar函数在TensorBoard查看损失函数值变化曲线。
局部极值点问题，通过权值随机初始化，增加靠近全局最优点附近开始下降机会。损失函数所有极值点接近等价。
tf.gradients方法，符号计算推导指定流图步骤梯度以张量输出。梯度下降法取决输入数据形状及问题特点。

误差反向传播算法，计算损失函数相对网络权值偏导，每层导数都是后一层导数与前一层导输出积。前馈，从输入开始，逐一计算隐含层输出，直到输出层。计算导数，从输出层逐一反向传播。复用所有已完成计算元素。

Sigmoid隐含层，softmax输出层以及带反向传播梯度下降，是最基础构件。

参考资料：
《面向机器智能的TensorFlow实践》


